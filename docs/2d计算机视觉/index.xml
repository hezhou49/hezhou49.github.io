<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2D计算机视觉 | FEZZY</title>
    <link>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</link>
      <atom:link href="https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/index.xml" rel="self" type="application/rss+xml" />
    <description>2D计算机视觉</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 14 Apr 2022 10:08:35 +0800</lastBuildDate>
    <image>
      <url>https://hezhou49.github.io/media/icon_hu1c8e88b8ad59f9a914752996b889ae01_459483_512x512_fill_lanczos_center_3.png</url>
      <title>2D计算机视觉</title>
      <link>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</link>
    </image>
    
    <item>
      <title>2D计算机视觉——原理、算法及应用</title>
      <link>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</link>
      <pubDate>Wed, 14 Apr 2021 02:23:00 +0800</pubDate>
      <guid>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</guid>
      <description>&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;《2D计算机视觉——原理、算法及应用》&lt;/p&gt;
&lt;h2 id=&#34;第3章-空域增强&#34;&gt;第3章 空域增强&lt;/h2&gt;
&lt;p&gt;算数运算：&lt;/p&gt;
&lt;p&gt;图像相减：体现两图的差别，可用于运动检测。&lt;/p&gt;
&lt;p&gt;对比度拉伸：增加原始图像里某两个灰度值的动态范围实现对比度的拉伸。&lt;/p&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220220192127924.png&#34; alt=&#34;image-20220220192127924&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;动态范围压缩：&lt;/p&gt;
&lt;p&gt;和对比度拉伸相反目的相反，看函数图就知道了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220220192318246.png&#34; alt=&#34;image-20220220192318246&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;直方图&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;使用直方图统计图像的亮度分布情况。&lt;/p&gt;
&lt;p&gt;直方图均匀化：使用累积分布函数CDF使得直方图分布均匀。&lt;/p&gt;
&lt;p&gt;直方图规定化：把直方图变成预期的形状。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;滤波&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;平滑（模糊）&lt;/p&gt;
&lt;p&gt;锐化&lt;/p&gt;
&lt;h2 id=&#34;第4章-频域增强&#34;&gt;第4章 频域增强&lt;/h2&gt;
&lt;p&gt;傅里叶变换在这个应用中的应用：把一个关于图像亮度的函数从空域（位置与亮度的关系）转化到频域（频率与亮度的关系）。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/21040374&#34;&gt;https://www.zhihu.com/question/21040374&lt;/a&gt;。连接中是从时域变换到频域。&lt;/p&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220221205234755.png&#34; alt=&#34;image-20220221205234755&#34; style=&#34;zoom: 80%;&#34; /&gt;
&lt;h2 id=&#34;第7章-图像分割&#34;&gt;第7章 图像分割&lt;/h2&gt;
&lt;p&gt;在对图像的研究和应用中，人们往往仅对图像中的某些部分感兴趣，这些部分常称为&lt;strong&gt;目标或前景&lt;/strong&gt;(其他部分称为&lt;strong&gt;背景&lt;/strong&gt;)，一般对应图像中特定的、具有独特性质的区域。为了辨识和分析目标，需要将这些区域分离并提取出来,如进行特征提取。&lt;/p&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220331164235340.png&#34; alt=&#34;image-20220331164235340&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;p&gt;先不管串、并行，分割的主要依据是&lt;strong&gt;检测边界&lt;/strong&gt;或者&lt;strong&gt;检测区域&lt;/strong&gt;，由此划分图像。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;微分边缘检测&#34;&gt;微分边缘检测&lt;/h3&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220331195105714.png&#34; alt=&#34;image-20220331195105714&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;p&gt;但是，图像是离散并非连续的，所以不能直接求导。所以数字图像的求导是利用&lt;strong&gt;差分来近似微分&lt;/strong&gt;的。&lt;/p&gt;
&lt;p&gt;二维的差分使用&lt;strong&gt;卷积&lt;/strong&gt;来实现。&lt;/p&gt;
&lt;p&gt;几个常用的梯度算子（一阶倒数算子/卷积模板）如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220331200445544.png&#34; alt=&#34;image-20220331200445544&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;基于灰度相似性的区域检测&#34;&gt;基于灰度相似性的区域检测&lt;/h3&gt;
&lt;h4 id=&#34;阈值分割&#34;&gt;阈值分割&lt;/h4&gt;
&lt;h4 id=&#34;区域生长&#34;&gt;区域生长&lt;/h4&gt;
&lt;p&gt;在实际应用区域生长技术时，需要解决三个问题:&lt;/p&gt;
&lt;p&gt;(1) 选择或确定一组能正确代表所需区域的种子像素。&lt;/p&gt;
&lt;p&gt;(2) 确定在生长过程中能将相邻像素包括进来的生长(或相似)准则。&lt;/p&gt;
&lt;p&gt;(3) 确定让生长过程停止的条件或规则。&lt;/p&gt;
&lt;h2 id=&#34;第8章-基元检测&#34;&gt;第8章 基元检测&lt;/h2&gt;
&lt;p&gt;基元检测本质上还是图像分割。&lt;/p&gt;
&lt;h3 id=&#34;角点检测&#34;&gt;角点检测&lt;/h3&gt;
&lt;h4 id=&#34;二阶导&#34;&gt;二阶导&lt;/h4&gt;
&lt;h4 id=&#34;哈里斯&#34;&gt;哈里斯&lt;/h4&gt;
&lt;p&gt;使用局部模板两个方向的梯度来定义。卷积的情况下，梯度的平方和也就是领域灰度值平方差的和。&lt;/p&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220503202812860.png&#34; alt=&#34;image-20220503202812860&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;h4 id=&#34;积分角点检测&#34;&gt;积分角点检测&lt;/h4&gt;
&lt;p&gt;使用圆形模板遍历每一个点，比较模板内其它点与中心点（核）的灰度值差异，小于阈值则记为“相同”，统计相同区域的面积比例。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220503204105437.png&#34; alt=&#34;image-20220503204105437&#34;&gt;&lt;/p&gt;
&lt;p&gt;可以发现，当中心点（核）恰好在角点上时，该面积可以取最小值，其它大多数时候面积大小都是大于2分之1的。&lt;/p&gt;
&lt;h3 id=&#34;哈夫变换&#34;&gt;哈夫变换&lt;/h3&gt;
&lt;p&gt;图像空间到参数空间的变换。&lt;/p&gt;
&lt;h2 id=&#34;滤波&#34;&gt;滤波&lt;/h2&gt;
&lt;p&gt;NR基础篇上——均值滤波、高斯滤波、双边滤波：https://zhuanlan.zhihu.com/p/422899763&lt;/p&gt;
&lt;p&gt;OpenCV双边滤波详解及实代码实现：https://blog.csdn.net/qq_36359022/article/details/80198890&lt;/p&gt;
&lt;h3 id=&#34;均值滤波&#34;&gt;均值滤波&lt;/h3&gt;
&lt;p&gt;太简单，略过。&lt;/p&gt;
&lt;h3 id=&#34;高斯滤波&#34;&gt;高斯滤波&lt;/h3&gt;
&lt;p&gt;使用高斯卷积核。&lt;/p&gt;
&lt;p&gt;当样本数据 X 是一维数据（Univariate）时，高斯分布遵从下方概率密度函数（Probability Density Function）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zhihu.com/equation?tex=P%28x%7C%5Ctheta%29+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%5E%7B2%7D%7D%7D+exp%28-%5Cfrac%7B%28x-%5Cmu%29%5E2%7D%7B2%5Csigma%5E%7B2%7D%7D%29&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;其中 &lt;img src=&#34;https://www.zhihu.com/equation?tex=%5Cmu&#34; alt=&#34;&#34;&gt; 为数据均值（期望）， &lt;img src=&#34;https://www.zhihu.com/equation?tex=%5Csigma&#34; alt=&#34;&#34;&gt; 为数据标准差（Standard deviation)，值越大，分散程度越大（峰越平缓)。&lt;/p&gt;
&lt;h3 id=&#34;双边滤波&#34;&gt;双边滤波&lt;/h3&gt;
&lt;p&gt;两个高斯权重相乘：
&lt;/p&gt;
$$
W S=e^{\left(-\frac{(i-k)^{2}+(j-l)^{2}}{2 \sigma_{s}^{2}}\right)}
$$
$$
w r=e^{\left(-\frac{\|f(i, j)-f(k, l)\|^{2}}{2 \sigma_{r}^{2}}\right)}
$$
&lt;p&gt;上面是二维的高斯分布，跟位置有关系，下面这个是图像的亮度差值的一维高斯分布。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/v2-916cb4a58d778a17052eab96d74884b5_b.jpg&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220707145424837.png&#34; alt=&#34;image-20220707145424837&#34;&gt;&lt;/p&gt;
&lt;p&gt;由于在空域，像素是整齐排列的，所以在函数上采样的ws（权重）是上图左侧均匀变化的，但是在color space亮度是没有规律的，所以就是越亮度或色彩越相似，权重越高；如果在边缘区域出现剧烈的亮度变化，那么不相似的一侧分配到的权重就会很小，于是出现上图所示的“断崖式”缩减。&lt;/p&gt;
&lt;p&gt;opencv给出的实现中，给出的双边滤波器bilateralFilter的3个主要参数如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;bilateralFilter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;opt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sigmaColor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sigmaSpace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt;/*
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt;d				邻域半径
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt;sigmaColor		color空间的sigma值，也就是高斯分布的标准差值，值越大也就是亮度差值大的也会获得较高权重；接近无穷大时，那么退化成普通高斯滤波。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt;sigmaSpace		普通的空域高斯函数的标准差值。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cm&#34;&gt;*/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>传统物体检测</title>
      <link>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BC%A0%E7%BB%9F%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B/</link>
      <pubDate>Wed, 14 Apr 2021 02:23:00 +0800</pubDate>
      <guid>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BC%A0%E7%BB%9F%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B/</guid>
      <description>&lt;h2 id=&#34;写在前面&#34;&gt;写在前面&lt;/h2&gt;
&lt;p&gt;如今，当讨论到物体检测时，通常理解的是“区域提议+物体识别”这个组合，零几年的时候出现了类似滑动窗口+HOG+SVM这种检测行人的方案，即做了特征提取后使用分类器将该特征进行分类，这就是传统物体检测的思路。后面深度学习也是这种思路，只是使用了深度神经网络代替传统方法实现了特征提取与分类。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;但是，在这种特征提取+分类的思路出现前莫非就不能做物体的识别和检测了？比如：下文提到的模板匹配的方法，模板匹配的关键是有一个或一组事先已经生成的模板。匹配模板图像与目标图像的特征点实现物体检测。特征的也有颜色、纹理、形状等多种定义。&lt;u&gt;比如《基于 Kinect 的物体分割与识别算法研究，D》中介绍了统计模板图像的颜色HSV空间分布直方图，H分为8个Bin，S和V各分为3个，由此形成72维特征向量的模板，在目标图像内先使用分割算法进行区域提议，然后将得到的区域进行颜色特征提取并于模板匹配。&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;所以，应该宽泛的理解“物体检测”，不是只有“特征提取+分类”这种思路，在很多简单的应用场景下可以使用比较简单的方法达到物体检测的目的。（比如之前识别可乐拉罐直接简单的识别红色色块，这不也是达到了物体检测的目的？）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;此处还是记录基于“特征提取+分类”这种通常意义中的物体识别。传统方法做“区域提议”思路也比较多、杂，比如：使用各种方法分割出固定区域、一些颜色聚类方法等，当然最简单还是滑动窗口，区域提议的方法不展开说，主要还是focus在“特征提取+分类”上，所以这篇文章本质上应该叫做传统物体识别。&lt;/p&gt;
&lt;h2 id=&#34;hogsvm&#34;&gt;HOG+SVM&lt;/h2&gt;
&lt;h2 id=&#34;harradaboost&#34;&gt;HARR+Adaboost&lt;/h2&gt;
&lt;h2 id=&#34;opencv-multiscale滑动窗口&#34;&gt;opencv multiScale滑动窗口&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>基于特征模板匹配的物体检测</title>
      <link>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9F%BA%E4%BA%8E%E7%89%B9%E5%BE%81%E6%A8%A1%E6%9D%BF%E5%8C%B9%E9%85%8D%E7%9A%84%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B/</link>
      <pubDate>Wed, 14 Apr 2021 02:23:00 +0800</pubDate>
      <guid>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9F%BA%E4%BA%8E%E7%89%B9%E5%BE%81%E6%A8%A1%E6%9D%BF%E5%8C%B9%E9%85%8D%E7%9A%84%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B/</guid>
      <description>&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;知乎专栏，opencv实现，40+文章，不错：https://www.zhihu.com/column/lowkeyway-OpenCV&lt;/p&gt;
&lt;p&gt;opencv docs——Feature Matching：https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html&lt;/p&gt;
&lt;p&gt;很早之前看过这个教程，但没想到这其实就是传统的基于2D图像特征模板做匹配实现物体识别的技术路线。&lt;/p&gt;
&lt;h2 id=&#34;主要思路&#34;&gt;主要思路&lt;/h2&gt;
&lt;p&gt;如今，当讨论到物体检测时，通常理解的是“区域提议+物体识别”这个组合，零几年的时候出现了类似滑动窗口+HOG+SVM这种检测行人的方案，即做了特征提取后使用分类器将该特征进行分类，这就是传统物体检测的思路。后面深度学习也是这种思路，只是使用了深度神经网络代替传统方法实现了特征提取与分类。&lt;/p&gt;
&lt;p&gt;但是，在这种特征提取+分类的思路出现前莫非就不能做物体的识别和检测了？比如：下文提到的模板匹配的方法，模板匹配的关键是有一个或一组事先已经生成的模板。匹配模板图像与目标图像的特征点实现物体检测。特征的也有颜色、纹理、形状等多种定义。&lt;u&gt;比如《基于 Kinect 的物体分割与识别算法研究，D》中介绍了统计模板图像的颜色HSV空间分布直方图，H分为8个Bin，区域各分为3个，由此形成72维特征向量的模板，在目标图像内先使用分割算法进行区域提议，然后将得到的区域进行颜色特征提取并于模板匹配。&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;所以，应该宽泛的理解“物体检测”，不是只有“特征提取+分类”这种思路，在很多简单的应用场景下可以使用比较简单的方法达到物体检测的目的。（比如之前识别可乐拉罐直接简单的识别红色色块，这不也是达到了物体检测的目的？）&lt;/p&gt;
&lt;h2 id=&#34;算法流程&#34;&gt;算法流程&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/matcher_flann.jpg&#34; alt=&#34;FLANN based matching&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;what we need？流程简述：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;目标物体模板：用一张（多张，maybe不同角度）仅包含目标物的图像进行特征点检测与描述，从而生成一组（多组）特征描述子模板。一组模板的shape为m X n，m为点特征点个数，n为特征向量的维度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;待查找图片（包含有目标物体）：对该图像进行特征点检测与描述，生成一组特征描述子。shape为k X n。k为特征点个数，n为特征向量的维度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;特征匹配过程：将模板与步骤2生成的特征描述子进行匹配（查找最近邻）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;计算单应性矩阵，实现目标物体图像到实际查找图像的转换。（最少需要4组点对计算单应矩阵，使用ransac方法对原始匹配进行误匹配剔除并计算最优转换）（这样就实现在检测图片出框选出了模板图片，如果还有对齐的深度图，可以直接在深度图上把物体抠出来）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/homography_findobj.jpg&#34; alt=&#34;Finding object with feature homography&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;几种2d特征点算法&#34;&gt;几种2D特征点算法&lt;/h2&gt;
&lt;p&gt;特征点=关键点（key points）+对应的特征描述（descriptor）&lt;/p&gt;
&lt;p&gt;机器视觉在工业机器人分拣系统中的应用[D]，2018，宋玉雪&lt;/p&gt;
&lt;p&gt;知乎专栏，三个算法都有讲解：https://www.zhihu.com/column/c_1296447643791560704&lt;/p&gt;
&lt;p&gt;SIFT算法：https://zhuanlan.zhihu.com/p/343522892（更清晰）&lt;/p&gt;
&lt;p&gt;SIFT特征详解 ：https://www.cnblogs.com/wangguchangqing/p/4853263.html&lt;/p&gt;
&lt;h3 id=&#34;sift&#34;&gt;SIFT&lt;/h3&gt;
&lt;p&gt;看上面链接吧，高斯尺度空间。&lt;/p&gt;
&lt;p&gt;128维特征描述。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要思路：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;为了提取尺度不变的特征点，在图像金字塔内进行特征点检测。早期的图像金字塔直接将图像不断降采样到原来的1/2，高斯尺度空间额外对同一大小的图像进行不同大小的高斯模糊（模拟人眼看物体的模糊程度）&lt;/p&gt;
&lt;p&gt;为了应对方向不变，计算特征点的邻域内的梯度方向与梯度值（使用直方图，360分为8个bin，每个bin45度，然后统计梯度值累加），统计最大的方向作为主方向。&lt;/p&gt;
&lt;p&gt;特征描述：选取16X16的邻域，选取4X4的格子作为种子邻域，同样也使用直方图统计每一个种子邻域的直方图，那么每个种子邻域形成8维向量，16个种子形成128维向量，然后再进行一行额外操作，比如归一化等。&lt;/p&gt;
&lt;h3 id=&#34;surf&#34;&gt;SURF&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Speeded  Up  Robust Features.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;（1）在生成尺度空间方面，SIFT中下一组图像的尺寸是上一组的一半，同一组间图像尺寸一样，但是所使用的高斯模糊系数逐渐增大。而在SURF中，&lt;strong&gt;不同组间图像的尺寸都是一致的&lt;/strong&gt;，但不同组间使用的盒式滤波器的模板尺寸逐渐增大，同一组间不同层间使用相同尺寸的滤波器，但是滤波器的模糊系数逐渐增大。&lt;/p&gt;
&lt;p&gt;（2）在特征点检验时，SIFT算子是先对图像进行非极大值抑制，再去除对比度较低的点。然后通过Hessian矩阵去除边缘的点。SURF算法是先通过Hessian矩阵来检测候选特征点，然后再对非极大值的点进行抑制。&lt;/p&gt;
&lt;p&gt;（3）在特征向量的方向确定上，SIFT算法是在正方形区域内统计梯度的幅值的直方图，找到最大梯度幅值所对应的方向。SIFT算子确定的特征点可以有一个或一个以上方向，其中包括一个主方向与多个辅方向。SURF算法则是在圆形邻域内，检测&lt;strong&gt;各个扇形范围内水平、垂直方向上的Haar小波响应&lt;/strong&gt;，找到模值最大的扇形指向，且该算法的方向只有一个。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220709160518873.png&#34; alt=&#34;image-20220709160518873&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;（4）SIFT算法生成描述子时，是将16x16的采样点划分为4x4的区域，从而计算每个分区种子点的幅值并确定其方向，共计4x4x8=128维。SURF算法在生成特征描述子时将的正方形分割成4x4的小方格，每个子区域25个采样点，计算小波haar响应，一共4x4x4=64维。&lt;/p&gt;
&lt;p&gt;综上，SURF算法在各个步骤上都简化了一些繁琐的工作，仅仅计算了特征点的一个主方向，生成的特征描述子也与前者相比降低了维数。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220709160136397.png&#34; alt=&#34;image-20220709160136397&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;orb&#34;&gt;ORB&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Oriented FAST and Rotated BRIEF ，改进了FAST角点与BRIEF描述子&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Fast是一个非常快速的关键点检测算法（不带特征描述），思路是直接比较当前点与邻域点的亮度差异。没有什么求导、积分、卷积操作，只比较亮度大小，所以很快。流程如下（图源视觉SLAM 14讲）：&lt;/p&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220708165659707.png&#34; alt=&#34;image-20220708165659707&#34; style=&#34;border:1px solid&#34; /&gt;
&lt;p&gt;&lt;strong&gt;关键点：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;但是FAST关键点没有尺度和方向，使用图像金字塔（简单的上、下采样）解决尺度问题，方向是通过灰度质心法解决。&lt;/p&gt;
&lt;p&gt;灰度质心法顾名思义是计算邻域内灰度的质心，公式很简单，就是加权平均。
&lt;/p&gt;
$$
x_0=\frac {\sum_{x, y \in B} x I(x, y)} {\sum_{x, y \in B}  I(x, y)}, y_0=\frac {\sum_{x, y \in B} y I(x, y)} {\sum_{x, y \in B}  I(x, y)}
$$
&lt;p&gt;
将当前点与质心进行连线，就生成了该关键点的方向了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特征描述：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;特征点检测-ORB：https://zhuanlan.zhihu.com/p/91479558&lt;/p&gt;
&lt;p&gt;邻域内随机选取两点，比较点的亮度强弱关系，从而赋值0或1。选取N=256组，形成256维向量。上述链接为数不多的介绍了brief描述子的特征匹配规则，及使用汉明距离，因为每一维要么是0要么是1，所以距离的理论值是[0, N]&lt;/p&gt;
&lt;p&gt;为了解决旋转不变性，使用关键点提取部分得到的方向旋转邻域，然后再计算描述子。&lt;/p&gt;
&lt;p&gt;为了优化性能，其实还做了一步使用贪心算法的优化，但不过多介绍。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>深度学习物体检测</title>
      <link>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B/</link>
      <pubDate>Wed, 14 Apr 2021 02:23:00 +0800</pubDate>
      <guid>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B/</guid>
      <description>&lt;h1 id=&#34;2d&#34;&gt;2D&lt;/h1&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;教学视频与文档&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;三维点云处理技术，深蓝学院，Lecture 6-3D Obeject detection，&lt;a href=&#34;&#34;&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cs231n, Detection and Segmentation（19年密歇根拓展版 lecture 15&amp;amp;16）。&lt;a href=&#34;http://cs231n.stanford.edu/slides/&#34;&gt;slides&lt;/a&gt;，&lt;a href=&#34;https://www.bilibili.com/video/BV1hi4y1t7kF?p=31&amp;amp;vd_source=02a668bab41ac04d089449d71a07db41&#34;&gt;b站视频包含17年斯坦福以及19年密歇根&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;讲得最细，不用看17版cs231n了：cs231n 密歇根2019，&lt;a href=&#34;https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/schedule.html&#34;&gt;pages&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;lecture 15-object detection PDF&lt;/a&gt;，b站链接就在上面。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;代码与结构剖析&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/31426458&#34;&gt;一文读懂Faster RCNN&lt;/a&gt;：里面也包含了代码讲解&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_object_detection&#34;&gt;https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_object_detection&lt;/a&gt;：一个图像检测库，可以直接跑训练和测试，代码是torch官方代码加注释，虽然不是最新代码，但是几乎一样。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection/blob/master/docs/zh_cn/article.md&#34;&gt;https://github.com/open-mmlab/mmdetection/blob/master/docs/zh_cn/article.md&lt;/a&gt;：mmdetection教程与实战。（讲得也比较详细）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;物体检测里程&#34;&gt;物体检测里程&lt;/h2&gt;
&lt;p&gt;图源见下标，这篇论文也被CS231n提到。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220420110503221.png&#34; alt=&#34;image-20220420110503221&#34;&gt;&lt;/p&gt;
&lt;p&gt;检测=定位+分类&lt;/p&gt;
&lt;p&gt;引用参考 截至2022-04&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;name&lt;/th&gt;
&lt;th&gt;year&lt;/th&gt;
&lt;th&gt;cition&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;r-cnn&lt;/td&gt;
&lt;td&gt;2014 CVPR&lt;/td&gt;
&lt;td&gt;2.4W&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fast r-cnn&lt;/td&gt;
&lt;td&gt;2015 CVPR&lt;/td&gt;
&lt;td&gt;1.9W&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet&lt;/td&gt;
&lt;td&gt;2016 CVPR&lt;/td&gt;
&lt;td&gt;11W&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mask r-cnn&lt;/td&gt;
&lt;td&gt;2017 CVPR&lt;/td&gt;
&lt;td&gt;1.8W&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;CNN处理图像输入一个feature map（H X W X C），现在都喜欢这么干，在感受域不断变大的过程中，H、W不断变小。通常CNN是块结构，包含标准的卷积操作、max pool、resnet结构。&lt;/p&gt;
&lt;h2 id=&#34;two-stage&#34;&gt;two stage&lt;/h2&gt;
&lt;p&gt;深度学习物体检测的基础是我们已经掌握了深度学习图像识别问题。&lt;/p&gt;
&lt;p&gt;所以一种可能的物体检测思路是：在原始图像上通过一些方法框选出一部分区域（该方法称为region proposal），直接丢给识别网络，就能实现物体检测。框选取（region proposal）的策略中，一个简单而暴力的方法就是——使用滑动窗口，使用不同大小的窗口从左上滑倒右下，对每一个窗口都塞进网络进行一次分类识别。&lt;/p&gt;
&lt;p&gt;显然这种方法是很耗时，就是最简单暴力法，目标物体的数量、位置、大小（比例）在图中都是未知的，暴力滑动窗口的步幅和框大小等都需要调整，也因此会产生巨量的框。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220612174908366.png&#34; alt=&#34;image-20220612174908366&#34;&gt;&lt;/p&gt;
&lt;p&gt;来看看怎样一步步改进。&lt;/p&gt;
&lt;h3 id=&#34;rcnn&#34;&gt;RCNN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220612181803718.png&#34; alt=&#34;image-20220612181803718&#34;&gt;&lt;/p&gt;
&lt;p&gt;RCNN就是提出了一种region proposal的方法（非深度学习，selective search，原理是颜色聚类）来生成候选框，大概每张图像生成2k个候选框，然后再将候选框resize到统一大小丢尽网络，一个head做分类，一个head做x,y,w,h的回归。&lt;/p&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220612180756275.png&#34; alt=&#34;image-20220612180756275&#34; style=&#34;border: 1px solid;&#34; /&gt;
&lt;p&gt;问题来了，虽然现在这种方式已经比暴力方法快很多了，但是还是很慢。因为几乎是把crop的图片丢尽了一整个网络2000次，一张图片的运行时间在47s（当时）这个量级。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;思考如何改进：这2000张输入图片是有大量重叠的，所以说网络其实进行了很多重复的计算。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;fast-rcnn&#34;&gt;Fast RCNN&lt;/h3&gt;
&lt;p&gt;所以，为了避免上述这部分重复计算，Fast RCNN直接把原始图像先丢尽CNN得到一个feature map，再在feature map上做裁剪，这样就避免掉了大量重复的区域进入网络被重复计算。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220612211210540.png&#34; alt=&#34;image-20220612211210540&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Q: feature map长宽与原图长宽不一致，那么如何做裁剪？以及裁剪出来后怎么变成大小一致？&lt;/p&gt;
&lt;p&gt;A: 使用ROI Pooling&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;roi-pooling&#34;&gt;ROI Pooling&lt;/h4&gt;
&lt;p&gt;ROI Pooling过程示意如下：经过初始CNN后，经由3X640X480变成了512X20X15的feature map。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220612211612193.png&#34; alt=&#34;image-20220612211612193&#34;&gt;&lt;/p&gt;
&lt;p&gt;对feature map每一个通道：框等比例缩小后，框先停靠在就近像素点上（图右上红变黑），假如是2X2的max pool，那么每个方向上均分格子（整数），然后每个格子内做max pool。这就是ROI Pooling，实际上很简单。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220612212724652.png&#34; alt=&#34;image-20220612212724652&#34;&gt;&lt;/p&gt;
&lt;p&gt;但是实际上由红框变黑框这样是有精度损失的，最后从feature map尺度变回原图时，这个放缩过程也将会带来一定的精度损失。所以提出了ROI Align。&lt;/p&gt;
&lt;h4 id=&#34;roi-align&#34;&gt;ROI Align&lt;/h4&gt;
&lt;p&gt;ROI Align是在Mask RCNN内被提出的。核心的思路就是一切都采用浮点数来表示。如下图所示：要把本来大小是6.25 X 4.53的边框统一到3 X 3。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220612214441058.png&#34; alt=&#34;image-20220612214441058&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;使用插值方法&lt;/strong&gt;：图中每一个角点的位置都是已知的，如图左下，在绿框（最终要resize成的3 X 3）内生成4个点（长宽等分3份）。如右下角图所示，然后这四个点的每一个点的值又由相邻四个feature map的像素值插值而得。最终，在将四个点做一个max操作就能得到3 X 3中的一个像素的值了。&lt;/p&gt;
&lt;h4 id=&#34;评价&#34;&gt;评价&lt;/h4&gt;
&lt;p&gt;总之，Fast RCNN首先将原图丢入卷积网络生成特征图，再在特征图上做region proposal，这样避免的不必要的重叠部分多次进入网络造成的重复计算，从而降低了时间开支。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220612215832083.png&#34; alt=&#34;image-20220612215832083&#34;&gt;&lt;/p&gt;
&lt;p&gt;但是，从上图可以看出，Fast RCNN已经够快了。但现在Fast RCNN的时间开销主要来自于传统的region proposal方法，改进这个非传统方法就给整体算法带来较大时间上的性能提升。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;所以，Faster RCNN应运而生，采用深度学习的方法代替传统方法。使用CNN做region proposal。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;faster-rcnn&#34;&gt;Faster RCNN&lt;/h3&gt;
&lt;p&gt;将fast rcnn中的传统region proposal方法替换为深度学习方法。叫做Region Proposal Network(RPN)。&lt;/p&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220420131808460.png&#34; alt=&#34;image-20220420131808460&#34; style=&#34;zoom:80%;border:1px solid&#34; /&gt;
&lt;blockquote&gt;
&lt;p&gt;那么RPN又是如何在feature map生成候选框的呢？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;anchor概念&#34;&gt;Anchor概念&lt;/h4&gt;
&lt;p&gt;对feature map上每一个点(格子或像素)生成多个不同大小和尺度（下图三个颜色代表三个尺度）的&amp;quot;Anchor&amp;quot;作为初始候选，判断该anchor内是否包含有目标（包含有目标的概率和不包含目标的概率），包含则继续预测从当前anchor到ground truth的4个值的偏差。&lt;/p&gt;
&lt;p&gt;下半图简单的把RPN当成是全连接网络举例，把C X anchor W X anchor H（加入是3X3 anchor）的向量拉成9C进入全连接网络输出为256维，然后外加两个做分类和回归的head。比如：分类问题用一个softmax来表示该anchor含有目标的概率和不包含目标的概率，所以输出的2*k个score，k为anchor数量。&lt;/p&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220612225550287.png&#34; alt=&#34;image-20220612225550287&#34; style=&#34;zoom:80%;border:1px solid&#34; /&gt;
&lt;p&gt;然后根据含有object的score排序，选择预测出的前300个region，即完成region proposal工作。如果总共是20X15规格的feature map，每个点6个anchor，那么初始的候选框其实也有2100种了（但是这2000个是以最简单的规则暴力生成的，不是复杂算法生成的，几乎不耗时）&lt;/p&gt;
&lt;h3 id=&#34;faster-rcnn代码剖析&#34;&gt;Faster RCNN代码剖析&lt;/h3&gt;
&lt;p&gt;基于&lt;a href=&#34;#Reference&#34;&gt;Reference&lt;/a&gt;里的几个链接，以及实际调试代码。&lt;/p&gt;
&lt;h4 id=&#34;整体框架&#34;&gt;整体框架&lt;/h4&gt;
&lt;p&gt;首先看一下这个整体架构图。&lt;/p&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/fasterRCNN.png&#34; alt=&#34;img&#34; style=&#34;border:1px solid&#34; /&gt;
&lt;p&gt;根据调试结构讲一下整个输入、输出。&lt;/p&gt;
&lt;p&gt;首先，整个代码的流程是在&lt;a href=&#34;https://github.com/pytorch/vision/blob/main/torchvision/models/detection/generalized_rcnn.py&#34;&gt;generalized_rcnn.py&lt;/a&gt;里，这是一个基类，FasterRCNN(GeneralizedRCNN)将集成它，可以点进去看看forward函数明确框出代码的大结构。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backbone&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;backbone&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rpn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rpn&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;roi_heads&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;roi_heads&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;继承时的默认参数都在这了&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220620224012662.png&#34; alt=&#34;image-20220620224012662&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;transform与backbone&#34;&gt;transform与Backbone&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;1.首先是transform与主干网络&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;如下图，假设batch size是4，图片大小是(375, 500)，他会把图片resize到minsize=800，maxsize=1333。800这个尺度基本上是一定的，另外一个在根据大小一般在1000-1300漂浮，并记录下原始的尺寸（最终检测结果再恢复回去）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;丢尽主干网络，原始的论文没有采用FPN，这里采用了FPN将输出5种大小尺度的特征图，通道数是256，批量大小是4。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220620224603719.png&#34; alt=&#34;image-20220620224603719&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;regionproposalnetwork&#34;&gt;RegionProposalNetwork&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;2.进入RPN，RegionProposalNetwork（时刻对照&lt;a href=&#34;#%E4%BB%A3%E7%A0%81%E5%89%96%E6%9E%90&#34;&gt;大图&lt;/a&gt;看）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注：RPN的最终目的是提供一推RegionProposal，相当于很多张在transform后的图像上的框。&lt;/p&gt;
&lt;p&gt;RPN网络部分涉及到网络训练，所以训练和推理的过程不同，训练或多出一些求loss的步骤，会额外说明。&lt;/p&gt;
&lt;p&gt;流程可见RegionProposalNetwork类的&lt;a href=&#34;https://github.com/pytorch/vision/blob/d0d7058a36fb4ea702dcb6c92c03d90ca91d8281/torchvision/models/detection/rpn.py#L339&#34;&gt;forward函数&lt;/a&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;RPN HEAD，进行分类（有无）与框回归。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 5张特征图&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;objectness&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pred_bbox_deltas&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在所有的feature map丢尽卷积网络，预设的是每个特征图上的一点对应会生成3个Anchor，所以需要把(4,256,200,304)的map卷积成(4,3,200,304)的分类和(4,3X4,200,304)的框回归。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220620231117190.png&#34; alt=&#34;image-20220620231117190&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;生成anchor，如上图所示，此时每张图片应生成的anchor的数量是3X(200X304+100X152+50X76+25X38+13X19)=242991，anchor的尺度也是在“原图”上生成的。将HEAD输出的4X242991个box作为原始的proposal。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接下来对proposal进行过滤。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;boxes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scores&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter_proposals&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;proposals&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;objectness&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;images&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;image_sizes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_anchors_per_level&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;对于每一张输入图片，首先根据HEAD输出的objectness，在其每张特征图上选取rpn_pre_nms_top_n_train个即头2000个box（Test是1000），由于最小的特征图只有13X19X3=741个proposal，所以总共在pre_nms阶段选出了8741个proposal，再对舍去小面积box，根据阈值移除score小的proposal，再根据超参数rpn_nms_thresh=0.7进行NMS（比如NMS后可能还剩下3200个），最后选取rpn_post_nms_top_n_train=2000选取score前2000的proposal。一张图片最终生成2000个proposal，4张即8000个。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220620235543673.png&#34; alt=&#34;image-20220620235543673&#34;&gt;&lt;/p&gt;
&lt;p&gt;如果是训练阶段，还要进行正负样本的判断并计算分类与回归的loss。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;计算每个anchor是正样本还是负样本，并计算box的偏移量。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;matched_gt_boxes&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;assign_targets_to_anchors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;anchors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;targets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;regression_targets&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;box_coder&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matched_gt_boxes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;anchors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 结合anchors以及对应的gt，计算regression参数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;根据超参数rpn_fg_iou_thresh=0.7，如果anchor与gt_boxes的iou大于0.7则是前景，根据rpn_bg_iou_thresh=0.3，小于0.3则是负样本。label的数量还是跟每张图anchor的总数量相同。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220621001307192.png&#34; alt=&#34;image-20220621001307192&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;选择预设数量的正、负样本计算loss。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss_objectness&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loss_rpn_box_reg&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;compute_loss&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;objectness&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pred_bbox_deltas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;regression_targets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# self.compute_loss 函数内&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;sampled_pos_inds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sampled_neg_inds&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fg_bg_sampler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;根据rpn_batch_size_per_image=256, rpn_positive_fraction=0.5，采样正负样本，即每张图像&lt;strong&gt;随机采样&lt;/strong&gt;（可以进代码看）128个正样本，128个负样本；4张图的话就是使用总共4X256=1024个样本计算loss。**注：**有可能一张图的正样本数量到不了128，那么就用负样本来补齐（如下，4张图才86个正样本，跑了下代码，感觉大多数时候都只有100个左右的正样本）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220621002826926.png&#34; alt=&#34;image-20220621002826926&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;roi-head&#34;&gt;ROI HEAD&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;每张图得到2000个proposal后，进入ROI HEAD&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ROI HEAD的训练和测试依然是有差异的，训练阶段的话是想得到比较好的loss所以选择部分proposal进行训练，不需要得到结果；而测试阶段不计算loss，直接把全部proposal丢进网络得出分类和回归结果，再进行包含有NMS的后处理得到最终结果。&lt;/p&gt;
&lt;p&gt;所以，以下完全分为train和Test来说：（对比RPN网络，不管是train还是test都是要得出proposal的，所以train比test前部分总体相同，train多出一个计算loss）&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;train：主要为了计算loss&lt;/th&gt;
&lt;th&gt;test：主要为了得出最终结果&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1.选择出部分proposal以及对应的label&lt;/td&gt;
&lt;td&gt;1.不操作，使用全部proposal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.ROI Align&lt;/td&gt;
&lt;td&gt;2.ROI Align&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3.丢尽网络得出预测的分类（目标种类）和框回归结果&lt;/td&gt;
&lt;td&gt;3.丢尽网络得出预测的分类（目标种类）和框回归结果&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4.计算loss&lt;/td&gt;
&lt;td&gt;4.包含NMS的后处理得出结果&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;code&gt;train：&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;得出proposal对应的label，并采样正负样本。&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 得到所有proposal的label&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;matched_idxs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;assign_targets_to_proposals&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;proposals&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gt_boxes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gt_labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 前景与背景的IOU&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 每张图从2000个采样至512个proposal，并且正样本最高占有1/4（因为大部分时候正样本数量不足）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;box_batch_size_per_image&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;512&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;box_positive_fraction&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.25&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;ROI Align。ROI的输入是proposal以及feature maps，根据proposal把特征图裁剪到7X7。注意观察下图中feature map变到box_features的变化，批量本来是4，然后每张图512个proposal，所以第一维度变成了4X512=2048，通道数不变，HXW变成了7X7&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220621151056504.png&#34; alt=&#34;image-20220621151056504&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;丢尽网络分类与回归。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 拉成一维并进入两层全连接层&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;box_features&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;box_head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;box_features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 分类、回归&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;class_logits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;box_regression&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;box_predictor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;box_features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;计算loss。两个loss，没什么好说的。加上RPN反馈的loss，总共就是4个loss，调试的这份代码是直接把所有loss加到一起进行梯度下降的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;test:&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;test阶段只会从RPN得到rpn_post_nms_top_n_test=1000个proposal！&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ROI Align，不具体说了，由于单图proposal有1000个，所以box_features应该是(4000,256,7,7)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;丢尽网络分类与回归。跟上面一样。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;后处理&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;boxes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scores&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;postprocess_detections&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;class_logits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;box_regression&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;proposals&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image_shapes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;对网络的预测数据进行后处理，包括
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;（1）根据proposal以及预测的回归参数计算出最终bbox坐标
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;（2）对预测类别结果进行softmax处理
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;（3）裁剪预测的boxes信息，将越界的坐标调整到图片边界上
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;（4）移除所有背景信息
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;（5）移除低概率目标                    box_score_thresh=0.05
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;（6）移除小尺寸目标
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;（7）执行nms处理，并按scores进行排序     box_nms_thresh=0.5
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;（8）根据scores排序返回前topk个目标      box_detections_per_img=100
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;家族对比&#34;&gt;家族对比&lt;/h3&gt;
&lt;h4 id=&#34;改进方向&#34;&gt;改进方向&lt;/h4&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220420152346937.png&#34; alt=&#34;image-20220420152346937&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;h4 id=&#34;速度比对&#34;&gt;速度比对&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220420131618419.png&#34; alt=&#34;image-20220420131618419&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;need-two-stage&#34;&gt;need two stage?&lt;/h4&gt;
&lt;p&gt;是否可以不要第二阶段？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220612220311614.png&#34; alt=&#34;image-20220612220311614&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;one-stage&#34;&gt;one stage&lt;/h2&gt;
&lt;p&gt;RPN HEAD和ROI HEAD完成的工作类似，所以现在网上教程一般都是先从one stage入手，再讲two stage。但是先学two stage家族有一个好处就是：可以清晰的看出物体检测这一任务从传统方法到深度学习方法的过渡，思路和逻辑都很明确。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>射影变换与二视图几何</title>
      <link>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%B0%84%E5%BD%B1%E5%8F%98%E6%8D%A2%E4%B8%8E%E4%BA%8C%E8%A7%86%E5%9B%BE%E5%87%A0%E4%BD%95/</link>
      <pubDate>Wed, 14 Apr 2021 02:23:00 +0800</pubDate>
      <guid>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%B0%84%E5%BD%B1%E5%8F%98%E6%8D%A2%E4%B8%8E%E4%BA%8C%E8%A7%86%E5%9B%BE%E5%87%A0%E4%BD%95/</guid>
      <description>&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;《计算机视觉中的多视图几何》 第二版 （太数学了）&lt;/li&gt;
&lt;li&gt;基于局部特征的图像与点云配准研究[D]&lt;/li&gt;
&lt;li&gt;《视觉SLAM 14讲》&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/102022619&#34;&gt;知乎：射影平面的直观理解&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zh.wikipedia.org/wiki/%E5%8D%95%E5%BA%94%E6%80%A7&#34;&gt;维基百科：单应性&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html&#34;&gt;opencv: Feature Matching + Homography to find Objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cnblogs.com/sunny-li/p/7500541.html&#34;&gt;二视图从运动到结构&amp;mdash;-对极几何和基础矩阵&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;射影空间与射影变换&#34;&gt;射影空间与射影变换&lt;/h3&gt;
&lt;h4 id=&#34;射影空间&#34;&gt;射影空间&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;齐次坐标&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220321114417446.png&#34; alt=&#34;image-20220321114417446&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;点、直线的齐次表示&lt;/strong&gt;&lt;/p&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220321115621439.png&#34; alt=&#34;image-20220321115621439&#34;  /&gt;
&lt;p&gt;&lt;strong&gt;射影空间&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;u&gt;n维射影空间用n+1维（除去全0）齐次向量组成。&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/102022619&#34;&gt;知乎：射影平面的直观理解&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;射影空间只是一组齐次向量的集合，它是为了方便运算而被制造出来，其实没有必要去研究他的物理或几何意义，重要的是关注射影变换&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;射影变换&#34;&gt;射影变换&lt;/h4&gt;
&lt;p&gt;射影空间是最&lt;strong&gt;一般&lt;/strong&gt;的空间变换。&lt;/p&gt;
&lt;p&gt;多视图几何几何给出的比较专业的定义如下，分为几何定义与代数定义。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/20220321152854.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://zh.wikipedia.org/wiki/%E5%8D%95%E5%BA%94%E6%80%A7&#34;&gt;维基百科：单应性&lt;/a&gt;在计算机视觉领域中，&lt;strong&gt;空间中同一平面的任意两幅图像&lt;/strong&gt;通过单应性关联在一起（假定是针孔相机）。比如,一个物体可以通过旋转相机镜头获取两张不同的照片(这两张照片的内容不一定要完全对应,部分对应即可),我们可以把单应性设为一个二维矩阵M,那么照片1乘以M就是照片2。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;基于局部特征的图像与点云配准研究[D]中，关于2D射影变换的描述：（其内容几乎来自多视图几何一书）&lt;/p&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220321150106448.png&#34; alt=&#34;image-20220321150106448&#34; style=&#34;zoom: 67%;&#34; /&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220321150136237.png&#34; alt=&#34;image-20220321150136237&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220321150512847.png&#34; alt=&#34;image-20220321150512847&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;关于透视变换（perspective）与射影变换（projective）的区别：很多地方并不区分透视变换与射影变换。&lt;/p&gt;
&lt;p&gt;透视变换是射影变换的一种特殊场景。（参考书P25）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220321155655449.png&#34; alt=&#34;image-20220321155655449&#34;&gt;&lt;/p&gt;
&lt;p&gt;区别：如图，两个透视变换的复合一般不是透视变换，而是射影变换。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220321160030395.png&#34; alt=&#34;image-20220321160030395&#34;&gt;&lt;/p&gt;
&lt;p&gt;比如opencv的&lt;strong&gt;getPerspectiveTransform&lt;/strong&gt;方法，字面意思就是改变给定图像的透视图。（是在改变给定的&lt;strong&gt;一张图&lt;/strong&gt;，射影变换通常是在两张图之间寻求变换（相当于更换相机位置重新拍了一张图），比如&lt;a href=&#34;https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html&#34;&gt;opencv: Feature Matching + Homography to find Objects&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220323202307667.png&#34; alt=&#34;image-20220323202307667&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# In Perspective Transformation, we can change the perspective of a given image or video for getting better insights into the required information.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;getPerspectiveTransform&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;method&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Parameters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Coordinates&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;quadrangle&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vertices&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;source&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;（&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;原图像四边形顶点&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;像素坐标&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;dst&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Coordinates&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;corresponding&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;quadrangle&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vertices&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;destination&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;（&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;期望的目标图像四边形顶点&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;像素坐标&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;所以，应该从上下文理解意思，不要太纠结字眼。&lt;strong&gt;透视&lt;/strong&gt;一词来自的中心投影，比如相机成像、针孔模型、人眼。（通常）&lt;/p&gt;
&lt;p&gt;快速分辨透视变换与射影变换。（图3是图像拼接）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220323204243319.png&#34; alt=&#34;image-20220323204243319&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;《视觉SLAM14讲》中也给出类似定义：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220321153212134.png&#34; alt=&#34;image-20220321153212134&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220321153251128.png&#34; alt=&#34;image-20220321153251128&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;求解&#34;&gt;求解&lt;/h4&gt;
&lt;p&gt;齐次方程有8个自由度，需要8个线性方程组，一组对应点能提供两个方程，至少需要4组对应点。&lt;/p&gt;
&lt;p&gt;使用cv2.getPerspectiveTransform提供精准四组点对可以求得变换矩阵。&lt;/p&gt;
&lt;p&gt;在特征匹配的场合，对应点可能出现无匹配，并且匹配对数可能远大于4对，可以使用&lt;a href=&#34;https://blog.csdn.net/fengyeer20120/article/details/87798638&#34;&gt;cv2.findHomography&lt;/a&gt;，使用最小均方误差或者RANSAC方法优化输出结果。&lt;/p&gt;
&lt;h3 id=&#34;摄像机模型&#34;&gt;摄像机模型&lt;/h3&gt;
&lt;p&gt;摄像机模型已经比较清楚了，相关的常规一点的链接比如。&lt;a href=&#34;https://blog.csdn.net/weixin_43206570/article/details/84797361&#34;&gt;相机模型 16个参数&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;参考书内使用齐次坐标详细推导了摄像机模型P的形成。参考书 #6.1&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220322111246449.png&#34; alt=&#34;image-20220322111246449&#34;&gt;&lt;/p&gt;
&lt;p&gt;需要注意的是，P是将3D投影到2D，是3X4矩阵，所以P是作用到世界坐标上的，x=PX。所以，R和t也是从&lt;strong&gt;世界坐标系到相机坐标系&lt;/strong&gt;的转换（相机坐标系是中间过程）。
&lt;/p&gt;
$$
\widetilde{\mathbf{X}}_{\mathrm{cam}}=\mathrm{R}(\widetilde{\mathbf{X}}-\widetilde{\mathbf{C}})
$$
&lt;p&gt;
其中，C~代表摄像机中心在世界坐标系中的坐标。&lt;/p&gt;
&lt;h4 id=&#34;求解-1&#34;&gt;求解&lt;/h4&gt;
&lt;p&gt;通过相机标定流程获取相机矩阵。&lt;/p&gt;
&lt;p&gt;求解外参可以使用PnP方法。[^PnP（ Perspective-n-Point）]是求解 3D 到 2D 点对运动的方法。它描述了当知道 n 个 3D 空间点及其投影位置时，如何估计相机的位姿。如果两张图像中其中一张特征点的 3D 位置已知，那么&lt;strong&gt;最少只需 3 个点对&lt;/strong&gt;（需要至少一个额外点验证结果）就可以估计相机运动。&lt;/p&gt;
&lt;p&gt;opencv提供了solvePnP函数来解决此问题。点对比较多的时候可以使用solvePnPRansac()消除噪声，提高鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/qq_30815237/article/details/87606687&#34;&gt;相机标定（3） opencv中solvePnPRansac()和solvePnP()计算外参数&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;二视图几何&#34;&gt;二视图几何&lt;/h3&gt;
&lt;h4 id=&#34;基本矩阵&#34;&gt;基本矩阵&lt;/h4&gt;
&lt;p&gt;基本矩阵是对极几何的代数表示。&lt;/p&gt;
&lt;p&gt;它表示在第一幅图像上的一个&lt;strong&gt;点&lt;/strong&gt;到另一幅图像上与之对应的对&lt;strong&gt;极线&lt;/strong&gt;的映射。&lt;strong&gt;是点到直线的映射。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/827012-20170910104028476-1239104682.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;性质&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220321171505518.png&#34; alt=&#34;image-20220321171505518&#34;&gt;&lt;/p&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220321172244630.png&#34; alt=&#34;image-20220321172244630&#34; style=&#34;zoom:90%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;求解&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;书上有介绍使用8点法或者7点法等数学方法求解F，但那是在未知相机参数的情况下。在双目相机的应用中，相机标定后，可以很轻松的使用上面红线的公式求出基本矩阵F。&lt;/p&gt;
&lt;h4 id=&#34;本质矩阵&#34;&gt;本质矩阵&lt;/h4&gt;
&lt;p&gt;博客讲得比较清楚。&lt;a href=&#34;https://www.cnblogs.com/sunny-li/p/7500541.html&#34;&gt;二视图从运动到结构&amp;mdash;-对极几何和基础矩阵&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;博客是从本质矩阵E讲到基本矩阵F的，参考书是先将F再讲E，不同的切入而已。&lt;/p&gt;
&lt;p&gt;注意，叉乘转矩阵运算的方法：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/827012-20170911213138000-1685547295.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;由基本矩阵恢复E：
&lt;/p&gt;
$$
\mathrm{E}=\mathrm{K}^{\prime \top} \mathrm{FK}
$$
&lt;p&gt;
&lt;strong&gt;从无标定参数的二试图求解本质矩阵：&lt;/strong&gt;（14讲——本质矩阵）&lt;/p&gt;
&lt;p&gt;本质矩阵只有5个自由度，照理说最少只需要5组点对即可求解，但是比较麻烦，所以通常使用8点法求解（8组）。（从工程实现角度考虑，由于平时通常会有几十对乃至上百对的匹配点，从 8 对减至 5 对意义并不明显。）&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>二视图三维重建与器械跟踪</title>
      <link>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BA%8C%E8%A7%86%E5%9B%BE%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E4%B8%8E%E5%99%A8%E6%A2%B0%E8%B7%9F%E8%B8%AA/</link>
      <pubDate>Wed, 14 Apr 2021 02:23:00 +0800</pubDate>
      <guid>https://hezhou49.github.io/docs/2d%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BA%8C%E8%A7%86%E5%9B%BE%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E4%B8%8E%E5%99%A8%E6%A2%B0%E8%B7%9F%E8%B8%AA/</guid>
      <description>&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;知乎主页：&lt;a href=&#34;https://www.zhihu.com/people/yingsongli&#34;&gt;李迎松&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GitHub：https://github.com/ethan-li-coding&lt;/li&gt;
&lt;li&gt;视频教程：&lt;a href=&#34;https://www.bilibili.com/video/BV1Uv411q7GU&#34;&gt;立体视觉之立体匹配理论与实战&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;手术器械的光学跟踪技术研究[D]，王志刚&lt;/li&gt;
&lt;li&gt;《视觉SLAM14讲》第7章&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/gangeqian2/article/details/119172447&#34;&gt;3D点集之间计算转移矩阵，旋转R，转移T，新增缩放s (总结全面)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;关于双目立体匹配与三维重建&#34;&gt;关于双目立体匹配与三维重建&lt;/h3&gt;
&lt;p&gt;一种分类方式是：稠密三维重建与稀疏三维重建。&lt;/p&gt;
&lt;p&gt;顾名思义，稀松三维重建是只讲视野内的部分点重建，稠密三维重建通常是将所有点的三维坐标都重建出来。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;我们的项目是稀疏点三维重建。两种方案很多基础知识和术语是相同的&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;目前，学界研究的基于双目的三维重建几乎都是稠密的深度图重建，因为说实话，稀疏点重建挺简单的。稀疏点的三维重建左右点的匹配关系可以做的非常精确，所以，通常情况下，稀疏点的三维重建的精度会比稠密三维重建高很多。&lt;/p&gt;
&lt;h3 id=&#34;稠密点三维重建&#34;&gt;稠密点三维重建&lt;/h3&gt;
&lt;h4 id=&#34;概述&#34;&gt;概述&lt;/h4&gt;
&lt;p&gt;从双目图像中恢复整张深度图（通常是以左摄像机坐标系为参考）。一些技术相似的任务：MVS、SFM&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220404225227485.png&#34; alt=&#34;image-20220404225227485&#34;&gt;&lt;/p&gt;
&lt;p&gt;场景：无人机、智能驾驶、机器人。&lt;/p&gt;
&lt;p&gt;方法：传统算法效果还不错，近几年出现大量基于深度学习的方法（以lidar值为groundtruth）。&lt;/p&gt;
&lt;h4 id=&#34;恢复相机运动&#34;&gt;&lt;strong&gt;恢复相机运动&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;对于MVS或者SFM等任务，没有相机间的坐标转换关系。对极约束简洁地给出了两个匹配点的空间位置关系。于是，相机位姿估计问题变为以下两步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据配对点的像素位置求出 E 或者 F。（E用8点法）&lt;/li&gt;
&lt;li&gt;根据 E 或者 F 求出 R、 t。（SVD，四组解，使用深度值为正的一组）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由于 E 和 F 只相差了相机内参，而内参在 SLAM 中通常是已知的，所以实践当中往往使用形式更简单的 E。&lt;/p&gt;
&lt;p&gt;除了基本矩阵和本质矩阵，二视图几何中还存在另一种常见的矩阵：单应矩阵（ Homography）H，它描述了两个平面之间的映射关系。若场景中的特征点都落在同一平面上（比如墙、地面等），则可以通过单应性来进行运动估计。&lt;/p&gt;
&lt;h4 id=&#34;方法框架&#34;&gt;方法框架&lt;/h4&gt;
&lt;p&gt;关于极限纠正的代码，opencv也提供了相应的接口，我们自己程序内也有，但是实际操作时没用而已。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220404224636480.png&#34; alt=&#34;image-20220404224636480&#34;&gt;&lt;/p&gt;
&lt;p&gt;右边是视差三维矩阵，横轴是像素坐标x,y，纵轴是视差d。C(x,y,d)记录的是左图(x,y)与右图(x+d,y)的匹配代价。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220405003528506.png&#34; alt=&#34;image-20220405003528506&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;稀疏点三维重建&#34;&gt;稀疏点三维重建&lt;/h3&gt;
&lt;p&gt;场景：刚体跟踪。&lt;/p&gt;
&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220405161616853.png&#34; alt=&#34;image-20220405161616853&#34; style=&#34;zoom: 67%;&#34; /&gt;
&lt;h4 id=&#34;匹配与重建&#34;&gt;匹配与重建&lt;/h4&gt;
&lt;p&gt;已实现具有精确的双目标定信息，所以只要左右对应点正确匹配，那么就可以使用&lt;strong&gt;三角测量&lt;/strong&gt;的方式将点精确重建。&lt;/p&gt;
&lt;p&gt;三维重建的任务是计算点的三维坐标，但我们的任务还要从点信息中恢复空间变换，所以实际上是一个&lt;strong&gt;刚体追踪&lt;/strong&gt;任务。需要确定哪些点是属于一个刚体的。所以这个问题不仅仅是立体匹配而已。关于点的分类，我们可以在二维图像中完成点的归类（比较困难），也可以在重建后（三维坐标）再对点进行分类。&lt;/p&gt;
&lt;p&gt;可以看出，点的归类与立体匹配其实是完全可以分开的两个任务。立体匹配只是为了找到精准的对应关系，要在这个环节创新的话，就是找到消除误匹配的方法。距离编码使用距离信息编码同时解决了误匹配和点排序问题。如果只是为了解决误匹配，可以直接只是用距离信息，无需编码。&lt;/p&gt;
&lt;p&gt;我个人觉得稀疏点这个任务是不太复杂的，因为我们手里的数据量其实也就只有那么几种：真实的点间距离，点的左右图像二维坐标，相机的标定参数。&lt;/p&gt;
&lt;h4 id=&#34;距离编码代码复现&#34;&gt;距离编码代码复现&lt;/h4&gt;
&lt;p&gt;这个方法是早先2012年王志刚的硕士毕业论文出现的，后面2016年才发小论文，小论文的数据和图是直接从大论文里面拿出来的，感觉是后来挤出来的小论文。发的&lt;strong&gt;中科院医学4区&lt;/strong&gt;Computer Assisted Surgery。&lt;/p&gt;
&lt;p&gt;手术器械的光学跟踪技术研究[D]-4.4&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://hezhou-img.oss-cn-shanghai.aliyuncs.com/img/image-20220405142439014.png&#34; alt=&#34;image-20220405142439014&#34;&gt;&lt;/p&gt;
&lt;p&gt;简述步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;进行编码，按照预先指定的点的顺序（比如第一个器械是前三个点&amp;hellip;）编码角点与距离，是&lt;strong&gt;一一对应&lt;/strong&gt;的（n个点）。&lt;/li&gt;
&lt;li&gt;执行对极匹配，输出所有的匹配关系（包含错误匹配），使用三角测量将每一对匹配重建为三维坐标（长度为m*m，m&amp;gt;n）。&lt;/li&gt;
&lt;li&gt;计算任意两点的欧式距离，填入矩阵形成m*m的上三角矩阵。&lt;/li&gt;
&lt;li&gt;遍历矩阵，判断值是否与真实的器械边长相等，则在新的m*m矩阵内填入对应的距离编码。遍历结束后，对称一下矩阵，将值填满。&lt;/li&gt;
&lt;li&gt;遍历每一行，使任意两个距离编码相加。如果等于角点编码，则匹配成功。（如果角点编码对应的是第4点，那么该点即为第4点）&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;随笔&#34;&gt;随笔&lt;/h4&gt;
&lt;p&gt;三维空间点中求空间变换，其实两个点就可以，总共6个自由度，两组点就能提供6个方程。（但是会出现方向歧义，绑定手术器械后，绕中心旋转180度，靶点位置没有变化，手术器械变了）？对吗，在思考一下。（SVD提供的对应点的集合，所以是否不存在这个问题？）（两点无法建立一个参考坐标系！无法规定x、y的方向，空间变换描述的是参考坐标系之间的变换）&lt;/p&gt;
&lt;p&gt;为什么空间中刚体跟踪通常使用的是三个点而不是两个点呢？因为两点只能绑定一条直线，无法描述一个平面，实际的刚体位置可以在绕该直线旋转的任意位置。&lt;/p&gt;
&lt;p&gt;所以两点之间的转换是能求的，但是两点不能绑定一个三维的刚体，因为两点无法创建坐标系，所以无法描述两点与刚体的固定转换。&lt;/p&gt;
&lt;p&gt;关于SVD，因为SVD的输入是对应点的集合，所以计算是没问题的。但是如何确定对应点关系呢？如果是2个点，从检测手段来看，你根本不知道哪个是点1哪个是点2。另外，不能是等腰三角形，这样绕中轴旋转180度靶点没有变。&lt;/p&gt;
&lt;p&gt;使用大小不一的靶点？&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
